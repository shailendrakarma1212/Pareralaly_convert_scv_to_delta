{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b742c8-5ce1-43b5-a7af-b73ee9aa9823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NOT_IMPLEMENTED Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7ffb27-70ea-4e8e-bc5d-b7cea629120d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read metadata from Unity Catalog table\n",
    "df_metadata = spark.table(\"my_unity_catalog_metastore.seq_exe_convert.file_matadata_1\")\n",
    "\n",
    "# Convert DataFrame to RDD of rows\n",
    "metadata_rdd = df_metadata.rdd\n",
    "\n",
    "# Function to perform conversion per row\n",
    "def convert_to_delta(row):\n",
    "    file_name = row[\"File Name\"]\n",
    "    source_path = row[\"Source Path\"]\n",
    "    delta_path = row[\"delta1_path\"]\n",
    "\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(source_path)\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "        print(f\"{file_name} successfully converted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {file_name}: {str(e)}\")\n",
    "\n",
    "# Run conversion in parallel\n",
    "metadata_rdd.foreach(convert_to_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff13af7-eafa-489a-94b2-cf087ed033c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefda334-5979-40b6-88c6-0f59a6df64c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load metadata\n",
    "df_meta = spark.table(\"my_unity_catalog_metastore.seq_exe_convert.file_matadata_1\")\n",
    "\n",
    "# Function to convert one file\n",
    "def convert_file(row):\n",
    "    try:\n",
    "        file_name = row[\"File Name\"]\n",
    "        source_path = row[\"Source Path\"]\n",
    "        # parquet_path = f\"{row['Parquet Path']}.{file_name}_parquet\"\n",
    "        delta1_path = f\"{row['delta1_path']}.{file_name}_delta\"\n",
    "\n",
    "        # Read source Delta table\n",
    "        df = spark.table(source_path)\n",
    "\n",
    "        # # Save as Parquet table\n",
    "        # df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(parquet_path)\n",
    "\n",
    "        # Save as Delta table\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta1_path)\n",
    "\n",
    "        print(f\" Converted: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error in {row['File Name']}: {e}\")\n",
    "\n",
    "# Run in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(convert_file, df_meta.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e7c7d8-074b-4c14-b07c-018200b7e797",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Parquet Path\":367},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754133493315}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.table(\"my_unity_catalog_metastore.seq_exe_convert.file_matadata_1\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22e8464-3165-4e93-8e5a-b3b5656e1615",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":25},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754131639405}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE my_unity_catalog_metastore.seq_exe_convert.file_matadata_1\n",
    "SET `Parquet Path` = 'workspace.default'\n",
    "WHERE `File Name` IN ('reviews', 'penguins', 'product_details');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d96f7bd-0558-42d6-9e5a-37e805ba4ec5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Parquet Path\":249},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754133500552}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from my_unity_catalog_metastore.seq_exe_convert.file_matadata_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86d4a57-5a94-4265-a232-59500ea0c5e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE my_unity_catalog_metastore.seq_exe_convert.execution_log(\n",
    "  file_name  String,\n",
    "  process_type  string,\n",
    "  start_time  timestamp,\n",
    "  end_time  timestamp ,\n",
    "  status  string,\n",
    "  error_message string \n",
    ") using delta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c6ed63-56e7-441e-925f-5c0ba57dc204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from my_unity_catalog_metastore.seq_exe_convert.execution_log as lo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac17515-3c8d-4e34-895a-ab02a491b8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Load metadata into list of Rows\n",
    "df_meta = spark.table(\"my_unity_catalog_metastore.seq_exe_convert.file_matadata_1\").collect()\n",
    "\n",
    "# Function to convert and log\n",
    "def convert_file(row):\n",
    "    file_name = row[\"File Name\"]\n",
    "    source_path = row[\"Source Path\"]\n",
    "    delta1_path = f\"{row['delta1_path']}.{file_name}_delta\"\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    status = \"SUCCESS\"\n",
    "    error_message = \"\"\n",
    "\n",
    "    try:\n",
    "        df = spark.table(source_path)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta1_path)\n",
    "        print(f\" Converted: {file_name}\")\n",
    "    except Exception as e:\n",
    "        status = \"FAILED\"\n",
    "        error_message = str(e)\n",
    "        print(f\" Error in {file_name}: {e}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    # Build log Row\n",
    "    log_row = Row(\n",
    "        file_name=file_name,\n",
    "        process_type=\"delta\",\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        status=status,\n",
    "        error_message=error_message\n",
    "    )\n",
    "\n",
    "    # Save log to Delta log table\n",
    "    spark.createDataFrame([log_row]).write.mode(\"append\").saveAsTable(\"my_unity_catalog_metastore.seq_exe_convert.execution_log\")\n",
    "\n",
    "# Run parallel execution\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(convert_file, df_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d529e9dd-02a2-4faa-84b5-f011e5e83a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from my_unity_catalog_metastore.seq_exe_convert.execution_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "339163a8-ce55-4efd-b558-058db7065d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6972893388911588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pareralaly convert scv files Notebook 2025-08-02 11:48:14",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
